\section{Code Overview}\label{code-overview}

A brief overview over the software architecture and design of the tool,
s.t. it becomes easier to navigate the code base and understand how the
different parts work together.

The tool is a command line application written in Java 8. Its most
important dependency is the GATE NLP framework.

\subsection{Package Overview}\label{package-overview}

Largest part of the Java code are in the package
\texttt{org.ratschlab.deidentifier}: * \texttt{annotation}: Language
analyzers (steps in the annnotation pipeline) * \texttt{dev}: Small
tools for sanity checks during development. Not used in production *
\texttt{pipelines}: Constructing and testing GATE pipelines for
deidentifiation * \texttt{sources}: Handling sources of reports.
Currently only reports from KISIM in JSON format *
\texttt{substitution}: implementation of different strategies to
substitute annotated tokens * \texttt{utils}: common utility functions *
\texttt{workflows}:

\subsection{NLP Pipeline
Implementation}\label{nlp-pipeline-implementation}

Annotating tokens to be deidentified happens over several steps forming
a pipeline. A GATE annotation pipeline
(\texttt{SerialAnalyserController}) consists of sequence of
\texttt{LanguageAnalyser}s. A document is passed along every analyser
and modified accordingly (e.g.~by adding certain annotations). Note,
that many concepts in GATE are represented by annotations, including
tokens and sentences.

The pipelines are constructed in a \texttt{PipelineFactory} in the
\texttt{org.ratschlab.deidentifier.pipelines} package. It adds analyser
steps based on the pipeline configuration. Many analysers could be
reused from GATE or GATE plugins, notably tokenization, sentence
splitting and lexica (gazeteer) annotations and JAPE rule engine.

\subsection{Aspects}\label{aspects}

\subsubsection{Configuration}\label{configuration}

Pipeline configurations are managed using a configuration file in
\href{https://github.com/lightbend/config}{HOCON format}. A pipeline
configuration file include paths to relevant files containing lexica,
specific JAPE rules, field lists for structured annotation etc

\subsubsection{Parallelization}\label{parallelization}

Documents can be processed in parallel by having several instances of
the annotation (or substitution) pipeline running in parallel.

This is managed by an Akka Stream compute flow/graph, which consist of
the following parts: * reading a document from a source (file or DB) and
convert it to a GATE document structure * distributing the document to
one of the annotation pipeline instance and annotate it * post process
single doc: e.g.~write to database or file * post process corpus:
e.g.~write doc stats to a single file, evaluate corpus

All these steps happen in a concurrent and asynchronic manner.

\subsubsection{Testing}\label{testing}

Code is tested using tests written in JUnit. Additionally, there is a
testing framework to test annotations. See \texttt{Testcases} section in
the components.md.
